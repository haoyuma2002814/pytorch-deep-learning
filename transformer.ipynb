{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b591b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(9.5), np.float64(9.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention (batch_first).\n",
    "    Input:  x  -> (B, T, D)\n",
    "    Output: y  -> (B, T, D)\n",
    "    Optional masks:\n",
    "      - attn_mask: (T, T) or (B, T, T) where True means \"mask out\" (disallow).\n",
    "      - key_padding_mask: (B, T) where True means \"this token is padding\" (disallow).\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.0, bias: bool = True):\n",
    "        super().__init__()\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(f\"embed_dim ({embed_dim}) must be divisible by num_heads ({num_heads}).\")\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
    "\n",
    "        # One big projection for Q, K, V (like PyTorch does)\n",
    "        self.qkv = nn.Linear(embed_dim, 3 * embed_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attn_mask: torch.Tensor | None = None,\n",
    "        need_weights: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)\n",
    "        attn_mask:\n",
    "          - (T, T) or (B, T, T), dtype bool or float\n",
    "            * bool: True = mask out\n",
    "            * float: additive mask (e.g., -inf for masked positions)\n",
    "        key_padding_mask:\n",
    "          - (B, T), dtype bool, True = padding token (mask out)\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        if D != self.embed_dim:\n",
    "            raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {D}.\")\n",
    "\n",
    "        # Project once, then split\n",
    "        qkv = self.qkv(x)  # (B, T, 3D)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # each (B, T, D)\n",
    "\n",
    "        # Reshape to heads: (B, H, T, d)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention scores: (B, H, T, T)\n",
    "        scores = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply attention mask (causal or custom)\n",
    "        if attn_mask is not None:\n",
    "            # Accept (T, T) or (B, T, T). We'll broadcast over heads.\n",
    "            if attn_mask.dim() == 2:\n",
    "                scores = scores.masked_fill(attn_mask[None, None, :, :], float(\"-inf\"))\n",
    "            elif attn_mask.dim() == 3:\n",
    "                scores = scores.masked_fill(attn_mask[:, None, :, :], float(\"-inf\"))\n",
    "            else:\n",
    "                raise ValueError(\"attn_mask must be (T,T) or (B,T,T) when bool.\")\n",
    "\n",
    "\n",
    "        # Softmax -> attention probs\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Weighted sum: (B, H, T, d)\n",
    "        y = attn @ v\n",
    "\n",
    "        # Merge heads back: (B, T, D)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        y = self.out_proj(y)\n",
    "\n",
    "        if need_weights:\n",
    "            # Often people return average attention over heads: (B, T, T)\n",
    "            return y, attn.mean(dim=1)\n",
    "        return y\n",
    "\n",
    "\n",
    "# ---- Example usage ----\n",
    "B = 32\n",
    "T = 10\n",
    "D = 100\n",
    "n = 10\n",
    "\n",
    "from torchinfo import summary\n",
    "attn = MultiHeadSelfAttention(embed_dim=D, num_heads=n, dropout=0.1)\n",
    "\n",
    "# Causal mask: prevent attending to future tokens\n",
    "\n",
    "causal = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "x = torch.randn(B,T,D)\n",
    "y, weight = attn(x, need_weights=True, attn_mask=causal)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.imshow(weight[0,...].cpu().detach())\n",
    "plt.axis('Off')\n",
    "\n",
    "summary(attn, input_size=[B, T, D])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
